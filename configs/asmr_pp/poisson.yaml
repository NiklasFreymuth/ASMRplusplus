name: "SLURM"   # MUST BE "SLURM"

# Required
partition: "multiple_il"
job-name: "poisson"    # this will be the experiments name in slurm
num_parallel_jobs: 99
time: 4300  # in minutes
cpus-per-task: 64
ntasks: 2
mem-per-cpu: 1000

sbatch_args: # Dictionary of SBATCH keywords and arguments
  distribution: cyclic  # To have repetitions of the same exp be distributed to different nodes
  nodes: 2

slurm_log: "./slurmlog"     # optional. dir in which slurm output and error logs will be saved.
sh_lines: [ "export WANDB_DIR=$TMPDIR/wandb", "mkdir $WANDB_DIR" ]
---

name: "DEFAULT"   # MUST BE DEFAULT
import_path: "asmr_default.yaml"
params:
  environment:
    mesh_refinement:
      element_penalty:
        min: 0.0005
        max: 0.01
      fem:
        pde_type: poisson
        domain:
          domain_type: lshape
          mean_hole_size: 0.15
          maximum_position_distortion: 0.3  # also doubles for the l-shape
      evaluation:
        fem:
          num_pdes: [ 1, 0 ]
          domain:
            fixed_domain: [ True, False ]
            maximum_position_distortion: [ 0.0, 0.3 ]
          poisson:
            fixed_load: [ True, False ]
      # final is set by default, I.e., we want to evaluate on the same setting that we use during training but with
      # new PDEs

---

name: poisson_asmr
list1:
  recording:
    idx: [ 1000 ]
list2:
  environment:
    mesh_refinement:
      element_penalty:
        max: [ 0.03 ]
        min: [ 0.0001 ]
---
name: poisson_asmr_generalizing
params:
  environment:
    mesh_refinement:
      element_penalty:
        max: 0.01
        min: 0.0001
      manual_normalization: 0.1  # manually set the normalization factor to 0.1 for a max error
      fem:
        num_pdes: -1
        poisson:
          boundary_type: "general"
          gmm_sample_mode: gaussian  # random, stratified or gaussian
        domain:
          domain_type: square_hole
      evaluation:
        manual_normalization: [ null ]
        fem:
          num_pdes: [ 1 ]
          domain:
            domain_type: [ lshape ]
            maximum_position_distortion: [ 0 ]
            fixed_domain: [ True ]
          poisson:
            boundary_type: [ "zero" ]
            gmm_sample_mode: [ random ]  # random, stratified or gaussian
            fixed_load: [ True ]
      final:
        manual_normalization: [ null ]
        fem:
          num_pdes: [ 100 ]
          domain:
            domain_type: [ lshape ]
            fixed_domain: [ False ]
          poisson:
            boundary_type: [ "zero" ]
            gmm_sample_mode: [ random ]  # random, stratified or gaussian

list:
  recording:
    idx: [ 1200 ]
---

#############
# Baselines #
#############

name: poisson_single_agent
params:
  algorithm:
    name: single_agent_ppo
    ppo:
      value_function_aggr: mean
      normalize_mappings: 0
    mixed_return:
      global_weight: 0.0
  environment:
    mesh_refinement:
      fem:
        error_metric: mean
      refinement_strategy: argmax  # refine exactly 1 element in every step as described in the argmax paper
      element_penalty:
        sample_penalty: False
        value: 0.0  # no face penalty since we refine exactly 1 element in every step in any case
      element_features:
        element_penalty: False
      reward_type: argmax  # reward type of the original baseline paper. No need to do area comparisons here

list1:
  recording:
    idx: [ 2000 ]
list2:
  environment:
    mesh_refinement:
      num_timesteps: [ 25, 50, 75, 100, 150,
                       200, 250, 300, 350, 400
      ]
---
name: poisson_vdgn

params:
  algorithm:
    name: ppo
    network:
      type_of_base: mpn
      training:
        learning_rate: 3.0e-4
    ppo:
      value_function_aggr: sum
      normalize_mappings: 0
    mixed_return:
      global_weight: 0.0
  environment:
    mesh_refinement:
      reward_type: vdgn
      fem:
        error_metric: mean
      element_penalty:
        min: 2.0e-5
        max: 5.0e-2
list1:
  recording:
    idx: [ 3000 ]

---
name: poisson_sweep

params:
  environment:
    environment_class: sweep_mesh_refinement
    mesh_refinement:
      element_penalty:
        value: 25
        sample_penalty: False
      fem:
        error_metric: mean
      num_training_timesteps: 200 # use different episode lengths for single agent training & multi-agent evaluation
      # here, we use 512 steps to give the agent enough time to learn the task
      num_timesteps: 6
      num_evaluation_timesteps: 6
      reward_type: sweep  # use the sweep reward

      element_features: # Sweep exclusive features.
        resource_budget: True # current number of elements / maximum number of elements
        average_error: False
        average_solution: True
        mean_area_neighbors: True # Agent gets mean area of neighbor elements
        mean_edge_attributes: True # Mean
        element_penalty: False
  algorithm:
    name: sweep_ppo
    network:
      type_of_base: sweep

    mixed_return:
      global_weight: 0.0
    ppo:
      value_function_aggr: mean
      num_rollout_steps: 512
      normalize_mappings: 0

list1:
  recording:
    idx: [ 4000 ]
list2:
  environment:
    mesh_refinement:
      maximum_elements: [ 200, 300, 400, 500, 750,
                          1000, 1500, 2000, 2500, 3000,
      ]
---
##############
# Heuristics #
##############
name: poisson_no_max_elements
# for the oracle, zz error and uniform refinement
repetitions: 1
reps_per_job: 1
reps_in_parallel: 1
iterations: 1
params:
  environment:
    mesh_refinement:
      maximum_elements: 1000000
      reward_type: spatial
      fem:
        error_metric: mean
---
name: poisson_oracle_maxheuristic
repetitions: 1
reps_per_job: 1
reps_in_parallel: 1
iterations: 1

params:
  environment:
    mesh_refinement:
      maximum_elements: 1000000
      fem:
        error_metric: maximum
